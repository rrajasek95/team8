{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "from seqeval.metrics import f1_score\n",
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_rev(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)][::-1]\n",
    "\n",
    "def tokenize_forw(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_forw, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_forw, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fields = [('ID', None), ('query', SRC), ('target', TRG)]\n",
    "train_data = TabularDataset(path=os.path.join('s2s_train.csv'), format='csv', skip_header=True, fields=data_fields)\n",
    "test_data = TabularDataset(path=os.path.join('s2s_test.csv'), format='csv', skip_header=True, fields=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': ['i', 'tend', 'to', 'agree', 'with', 'nyt', \"'s\", 'review', 'which', 'says', ':'], 'target': ['this', 'movie', 'is', 'brings', 'out', 'the', 'wholesome', ',', 'affirmative', 'side', 'of', 'the', 'hip', '-', 'hop', 'aesthetic', 'without', 'being', 'overly', 'preachy']}\n",
      "{'query': ['i', 'agree', ',', 'it', \"'s\", 'one', 'of', 'the', 'gems', 'that', 'we', 'get', 'each', 'year', '.'], 'target': ['i', 'love', 'that', 'part', 'when', 'honey', 'gets', 'a', 'bank', 'loan', ',', 'and', 'puts', 'down', 'a', 'deposit', '.', 'what', \"'s\", 'your', 'favorite', '?']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0]))\n",
    "print(vars(train_data.examples[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Data _ Towards Exploiting Background Knowledge for Building Conversation Systems (EMNLP 2018).zip'\r\n",
      " \u001b[0m\u001b[01;34mGlove\u001b[0m/\r\n",
      " \u001b[01;34mholle\u001b[0m/\r\n",
      "'Memory Network with Attention.ipynb'\r\n",
      " \u001b[01;34m__pycache__\u001b[0m/\r\n",
      " s2s_test.csv\r\n",
      " s2s_train.csv\r\n",
      " s2s_train_small.csv\r\n",
      " Seq2Seq-2.ipynb\r\n",
      " Seq2Seq.ipynb\r\n",
      " seq2seq.py\r\n",
      "'Transform data from HollE to HuggingFace.ipynb'\r\n",
      " tut1-model.pt\r\n",
      " Untitled.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "\n",
    "SRC.build_vocab(train_data, vectors=Vectors('Glove/glove.6B.100d.txt'))\n",
    "TRG.build_vocab(train_data, vectors=Vectors('Glove/glove.6B.100d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data, valid_data = train_data.split(split_ratio=0.7)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE, sort=False, sort_within_batch=False,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq-2 import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 100\n",
    "DEC_EMB_DIM = 100\n",
    "HID_DIM = 100\n",
    "N_LAYERS = 4\n",
    "ENC_DROPOUT = 0.2\n",
    "DEC_DROPOUT = 0.2\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, SRC.vocab.vectors)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, TRG.vocab.vectors)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(16304, 100)\n",
       "    (rnn): LSTM(100, 100, num_layers=2, dropout=0.2, bidirectional=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(23437, 100)\n",
       "    (rnn): LSTM(100, 100, num_layers=4, dropout=0.2)\n",
       "    (fc_out): Linear(in_features=100, out_features=23437, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 7,067,637 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                    mode='min', factor=0.5,\n",
    "                                                    patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    record_size = 0\n",
    "    out_preds = []\n",
    "    ground_preds = []\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        \n",
    "        src = batch.query\n",
    "        trg = batch.target\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        record_size += trg.shape[1]\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        out_preds.append(torch.transpose(torch.argmax(torch.softmax(output, dim=2), dim=2), 0,1).tolist())\n",
    "        ground_preds.append(torch.transpose(trg, 0,1).tolist())\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('train_loss :', epoch_loss/record_size, 'records :', record_size)\n",
    "            \n",
    "    ground_preds = flatten(ground_preds)\n",
    "    out_preds = flatten(out_preds)\n",
    "    \n",
    "    ground_tokens = []\n",
    "    for ex in ground_preds:\n",
    "        ground_tokens.append([TRG.vocab.itos[token] for token in ex])\n",
    "        \n",
    "    pred_tokens = []\n",
    "    for ex in out_preds:\n",
    "        pred_tokens.append([TRG.vocab.itos[token] for token in ex])\n",
    "        \n",
    "    return epoch_loss / record_size, f1_score(ground_tokens, pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    record_size = 0\n",
    "    out_preds = []\n",
    "    ground_preds = []\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    def devectorize(vec):\n",
    "      idxs = torch.nonzero(vec)\n",
    "      if len(idxs) == 0:\n",
    "        return 0\n",
    "      else:\n",
    "        sm = torch.softmax(vec, dim=0)\n",
    "        idx = sm.argmax(0)\n",
    "    #     print(idx)\n",
    "        return idx\n",
    "\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.query\n",
    "            trg = batch.target\n",
    "\n",
    "            record_size += trg.shape[1]\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "            out_preds.append(torch.transpose(torch.argmax(torch.softmax(output, dim=2), dim=2), 0,1).tolist())\n",
    "            ground_preds.append(torch.transpose(trg, 0,1).tolist())\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print('val_loss : ', epoch_loss/record_size, 'records :', record_size)\n",
    "\n",
    "    ground_preds = flatten(ground_preds)\n",
    "    out_preds = flatten(out_preds)\n",
    "    \n",
    "    ground_tokens = []\n",
    "    for ex in ground_preds:\n",
    "        ground_tokens.append([TRG.vocab.itos[token] for token in ex])\n",
    "        \n",
    "    pred_tokens = []\n",
    "    for ex in out_preds:\n",
    "        pred_tokens.append([TRG.vocab.itos[token] for token in ex])\n",
    "\n",
    "    return epoch_loss / record_size, f1_score(ground_tokens, pred_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss : 0.6288149356842041 records : 16\n",
      "train_loss : 0.45481122838388577 records : 1616\n",
      "train_loss : 0.4321626645712117 records : 3216\n",
      "train_loss : 0.42190233348215933 records : 4816\n",
      "train_loss : 0.4159653073088487 records : 6416\n",
      "train_loss : 0.41250308854375295 records : 8016\n",
      "train_loss : 0.41002835375497027 records : 9616\n",
      "train_loss : 0.40800518515956896 records : 11216\n",
      "train_loss : 0.4063236153825243 records : 12816\n",
      "train_loss : 0.4046390675810413 records : 14416\n",
      "train_loss : 0.40322299261431355 records : 16016\n",
      "train_loss : 0.4020212767992531 records : 17616\n",
      "train_loss : 0.40071226335087984 records : 19216\n",
      "train_loss : 0.3997367474319933 records : 20816\n",
      "train_loss : 0.398749068836754 records : 22416\n",
      "train_loss : 0.39795935775422003 records : 24016\n",
      "val_loss :  0.37450674176216125 records : 16\n",
      "val_loss :  0.3870857461254195 records : 1616\n",
      "val_loss :  0.38812942973416836 records : 3216\n",
      "val_loss :  0.3881105190495716 records : 4816\n",
      "val_loss :  0.3883895394659399 records : 6416\n",
      "val_loss :  0.3889379680989507 records : 8016\n",
      "val_loss :  0.38922881757756833 records : 9616\n",
      "Epoch: 01 | Time: 5m 45s\n",
      "\tTrain Loss: 0.397 | Train PPL:   1.488 | Train F1: 2.466\n",
      "\t Val. Loss: 0.389 |  Val. PPL:   1.476 | Train F1: 3.702\n",
      "train_loss : 0.35122784972190857 records : 16\n",
      "train_loss : 0.37756954207278715 records : 1616\n",
      "train_loss : 0.3774271549573585 records : 3216\n",
      "train_loss : 0.3781172143066444 records : 4816\n",
      "train_loss : 0.377100121350657 records : 6416\n",
      "train_loss : 0.37638665488856043 records : 8016\n",
      "train_loss : 0.37666423943594174 records : 9616\n",
      "train_loss : 0.37598253129722387 records : 11216\n",
      "train_loss : 0.37557148088825476 records : 12816\n",
      "train_loss : 0.3756247322580526 records : 14416\n",
      "train_loss : 0.37506110726536573 records : 16016\n",
      "train_loss : 0.3745798915218592 records : 17616\n",
      "train_loss : 0.374064290404419 records : 19216\n",
      "train_loss : 0.37350026045644585 records : 20816\n",
      "train_loss : 0.3735146123668622 records : 22416\n",
      "train_loss : 0.3731460429921617 records : 24016\n",
      "val_loss :  0.36912456154823303 records : 16\n",
      "val_loss :  0.3818228622474293 records : 1616\n",
      "val_loss :  0.3831458693713098 records : 3216\n",
      "val_loss :  0.3830122199169425 records : 4816\n",
      "val_loss :  0.3833853120072524 records : 6416\n",
      "val_loss :  0.3839380617389184 records : 8016\n",
      "val_loss :  0.3841695972964688 records : 9616\n",
      "Epoch: 02 | Time: 5m 41s\n",
      "\tTrain Loss: 0.373 | Train PPL:   1.452 | Train F1: 6.930\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468 | Train F1: 5.826\n",
      "train_loss : 0.34524813294410706 records : 16\n",
      "train_loss : 0.3643450229474814 records : 1616\n",
      "train_loss : 0.365050457751573 records : 3216\n",
      "train_loss : 0.3646724092405896 records : 4816\n",
      "train_loss : 0.3641713806369953 records : 6416\n",
      "train_loss : 0.3638390730360073 records : 8016\n",
      "train_loss : 0.363888305752924 records : 9616\n",
      "train_loss : 0.3638803024434839 records : 11216\n",
      "train_loss : 0.363408503423767 records : 12816\n",
      "train_loss : 0.363470340244778 records : 14416\n",
      "train_loss : 0.36311862140625983 records : 16016\n",
      "train_loss : 0.36273191924749126 records : 17616\n",
      "train_loss : 0.3625666439582863 records : 19216\n",
      "train_loss : 0.3624169893625789 records : 20816\n",
      "train_loss : 0.36215865926858276 records : 22416\n",
      "train_loss : 0.3621105492154731 records : 24016\n",
      "val_loss :  0.3675852119922638 records : 16\n",
      "val_loss :  0.3812529748619193 records : 1616\n",
      "val_loss :  0.38270918468930826 records : 3216\n",
      "val_loss :  0.3825761806529226 records : 4816\n",
      "val_loss :  0.38306940568058273 records : 6416\n",
      "val_loss :  0.3836405821546109 records : 8016\n",
      "val_loss :  0.3839096913916894 records : 9616\n",
      "Epoch: 03 | Time: 5m 40s\n",
      "\tTrain Loss: 0.362 | Train PPL:   1.436 | Train F1: 7.656\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.468 | Train F1: 5.524\n",
      "train_loss : 0.348520427942276 records : 16\n",
      "train_loss : 0.35600311626302134 records : 1616\n",
      "train_loss : 0.35658070949179616 records : 3216\n",
      "train_loss : 0.35629192915469704 records : 4816\n",
      "train_loss : 0.35647154679322185 records : 6416\n",
      "train_loss : 0.3555279380666044 records : 8016\n",
      "train_loss : 0.3545892656146984 records : 9616\n",
      "train_loss : 0.3546309774680417 records : 11216\n",
      "train_loss : 0.35463101884696663 records : 12816\n",
      "train_loss : 0.35461885018168754 records : 14416\n",
      "train_loss : 0.3544433403324771 records : 16016\n",
      "train_loss : 0.3547014688005456 records : 17616\n",
      "train_loss : 0.35461008538810734 records : 19216\n",
      "train_loss : 0.35446472801583073 records : 20816\n",
      "train_loss : 0.35455279685104857 records : 22416\n",
      "train_loss : 0.3546745651051015 records : 24016\n",
      "val_loss :  0.36549437046051025 records : 16\n",
      "val_loss :  0.3805647374379753 records : 1616\n",
      "val_loss :  0.38195025401922006 records : 3216\n",
      "val_loss :  0.38179391147290354 records : 4816\n",
      "val_loss :  0.3822813850091283 records : 6416\n",
      "val_loss :  0.38275898900812494 records : 8016\n",
      "val_loss :  0.38300988851688467 records : 9616\n",
      "Epoch: 04 | Time: 5m 41s\n",
      "\tTrain Loss: 0.355 | Train PPL:   1.426 | Train F1: 7.457\n",
      "\t Val. Loss: 0.383 |  Val. PPL:   1.467 | Train F1: 6.521\n",
      "train_loss : 0.3287379741668701 records : 16\n",
      "train_loss : 0.3506416681379375 records : 1616\n",
      "train_loss : 0.34971186162820506 records : 3216\n",
      "train_loss : 0.3488149610270693 records : 4816\n",
      "train_loss : 0.3491663593901066 records : 6416\n",
      "train_loss : 0.3490556433291254 records : 8016\n",
      "train_loss : 0.34900615496960735 records : 9616\n",
      "train_loss : 0.34877750987503225 records : 11216\n",
      "train_loss : 0.3490872290473752 records : 12816\n",
      "train_loss : 0.3493952342990236 records : 14416\n",
      "train_loss : 0.3493046662011942 records : 16016\n",
      "train_loss : 0.34895527996224346 records : 17616\n",
      "train_loss : 0.3487119399736962 records : 19216\n",
      "train_loss : 0.34868445781723156 records : 20816\n",
      "train_loss : 0.3488656182232965 records : 22416\n",
      "train_loss : 0.34882502485163447 records : 24016\n",
      "val_loss :  0.3591618537902832 records : 16\n",
      "val_loss :  0.3811262459448068 records : 1616\n",
      "val_loss :  0.38251283838974304 records : 3216\n",
      "val_loss :  0.3823657833856602 records : 4816\n",
      "val_loss :  0.3828349567559592 records : 6416\n",
      "val_loss :  0.383278870178078 records : 8016\n",
      "val_loss :  0.3835178449030922 records : 9616\n",
      "Epoch: 05 | Time: 5m 43s\n",
      "\tTrain Loss: 0.349 | Train PPL:   1.417 | Train F1: 7.499\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.467 | Train F1: 7.206\n",
      "train_loss : 0.35228005051612854 records : 16\n",
      "train_loss : 0.3449086296086264 records : 1616\n",
      "train_loss : 0.346383907901707 records : 3216\n",
      "train_loss : 0.34504341841536107 records : 4816\n",
      "train_loss : 0.3442541687863129 records : 6416\n",
      "train_loss : 0.3443483557410821 records : 8016\n",
      "train_loss : 0.3445347850017262 records : 9616\n",
      "train_loss : 0.344189610954018 records : 11216\n",
      "train_loss : 0.3438727708345049 records : 12816\n",
      "train_loss : 0.34390709290361565 records : 14416\n",
      "train_loss : 0.34398671154018406 records : 16016\n",
      "train_loss : 0.34391067777299317 records : 17616\n",
      "train_loss : 0.3437999342502305 records : 19216\n",
      "train_loss : 0.3437295357920773 records : 20816\n",
      "train_loss : 0.3436791195050892 records : 22416\n",
      "train_loss : 0.343732758671502 records : 24016\n",
      "val_loss :  0.3555227518081665 records : 16\n",
      "val_loss :  0.3818066131950605 records : 1616\n",
      "val_loss :  0.3832163447467842 records : 3216\n",
      "val_loss :  0.38313830126559617 records : 4816\n",
      "val_loss :  0.383594374555602 records : 6416\n",
      "val_loss :  0.38403165941228884 records : 8016\n",
      "val_loss :  0.38426063575482805 records : 9616\n",
      "Epoch: 06 | Time: 5m 44s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.410 | Train F1: 7.634\n",
      "\t Val. Loss: 0.384 |  Val. PPL:   1.469 | Train F1: 5.812\n",
      "train_loss : 0.36165153980255127 records : 16\n",
      "train_loss : 0.33906797489317336 records : 1616\n",
      "train_loss : 0.3390314212782466 records : 3216\n",
      "train_loss : 0.33831246568515055 records : 4816\n",
      "train_loss : 0.33857482485937657 records : 6416\n",
      "train_loss : 0.3391583251976919 records : 8016\n",
      "train_loss : 0.339054958693398 records : 9616\n",
      "train_loss : 0.33909200427365543 records : 11216\n",
      "train_loss : 0.3387114287315683 records : 12816\n",
      "train_loss : 0.3386956341854079 records : 14416\n",
      "train_loss : 0.33900081527816667 records : 16016\n",
      "train_loss : 0.33914789652196414 records : 17616\n",
      "train_loss : 0.33935234402240466 records : 19216\n",
      "train_loss : 0.33948439682657033 records : 20816\n",
      "train_loss : 0.3392703063908345 records : 22416\n",
      "train_loss : 0.33905598650289964 records : 24016\n",
      "val_loss :  0.35506683588027954 records : 16\n",
      "val_loss :  0.3820081364990461 records : 1616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss :  0.38342178653721787 records : 3216\n",
      "val_loss :  0.3834229557221118 records : 4816\n",
      "val_loss :  0.3838606479756553 records : 6416\n",
      "val_loss :  0.38436240725174636 records : 8016\n",
      "val_loss :  0.38456738084405906 records : 9616\n",
      "Epoch: 07 | Time: 5m 40s\n",
      "\tTrain Loss: 0.339 | Train PPL:   1.404 | Train F1: 7.590\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.469 | Train F1: 6.573\n",
      "train_loss : 0.3199482858181 records : 16\n",
      "train_loss : 0.33482715162900417 records : 1616\n",
      "train_loss : 0.3372305149759226 records : 3216\n",
      "train_loss : 0.3369194121653851 records : 4816\n",
      "train_loss : 0.33666210437653366 records : 6416\n",
      "train_loss : 0.3368671853504257 records : 8016\n",
      "train_loss : 0.3366155875304376 records : 9616\n",
      "train_loss : 0.33630806437232524 records : 11216\n",
      "train_loss : 0.33646155400817906 records : 12816\n",
      "train_loss : 0.33662181057358426 records : 14416\n",
      "train_loss : 0.33635893291407654 records : 16016\n",
      "train_loss : 0.33644561839038734 records : 17616\n",
      "train_loss : 0.33669838942060065 records : 19216\n",
      "train_loss : 0.33683386731386 records : 20816\n",
      "train_loss : 0.3368181378223656 records : 22416\n",
      "train_loss : 0.33672409318511604 records : 24016\n",
      "val_loss :  0.35583722591400146 records : 16\n",
      "val_loss :  0.3823177652193768 records : 1616\n",
      "val_loss :  0.38396056626566605 records : 3216\n",
      "val_loss :  0.38395785424004364 records : 4816\n",
      "val_loss :  0.3843512934936847 records : 6416\n",
      "val_loss :  0.3848615875858033 records : 8016\n",
      "val_loss :  0.3850896942139465 records : 9616\n",
      "Epoch: 08 | Time: 5m 43s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.400 | Train F1: 7.762\n",
      "\t Val. Loss: 0.385 |  Val. PPL:   1.470 | Train F1: 5.663\n",
      "train_loss : 0.33042559027671814 records : 16\n",
      "train_loss : 0.3322312241733664 records : 1616\n",
      "train_loss : 0.3330204125067488 records : 3216\n",
      "train_loss : 0.33236217102735144 records : 4816\n",
      "train_loss : 0.3335651256645707 records : 6416\n",
      "train_loss : 0.3337561081269544 records : 8016\n",
      "train_loss : 0.3335929233301896 records : 9616\n",
      "train_loss : 0.33342305191403276 records : 11216\n",
      "train_loss : 0.33370566609944596 records : 12816\n",
      "train_loss : 0.333454878130446 records : 14416\n",
      "train_loss : 0.3335958206272506 records : 16016\n",
      "train_loss : 0.33361439551687805 records : 17616\n",
      "train_loss : 0.3338065448152731 records : 19216\n",
      "train_loss : 0.3338642980070136 records : 20816\n",
      "train_loss : 0.3338850345635397 records : 22416\n",
      "train_loss : 0.3338464956971346 records : 24016\n",
      "val_loss :  0.358010858297348 records : 16\n",
      "val_loss :  0.3831061036279886 records : 1616\n",
      "val_loss :  0.384627647660858 records : 3216\n",
      "val_loss :  0.38467086094162395 records : 4816\n",
      "val_loss :  0.38505689706885604 records : 6416\n",
      "val_loss :  0.3855237572968839 records : 8016\n",
      "val_loss :  0.3857456367840981 records : 9616\n",
      "Epoch: 09 | Time: 5m 41s\n",
      "\tTrain Loss: 0.334 | Train PPL:   1.397 | Train F1: 7.835\n",
      "\t Val. Loss: 0.386 |  Val. PPL:   1.471 | Train F1: 6.360\n",
      "train_loss : 0.3605324923992157 records : 16\n",
      "train_loss : 0.33696850426126235 records : 1616\n",
      "train_loss : 0.3346021741776917 records : 3216\n",
      "train_loss : 0.3332298433661857 records : 4816\n",
      "train_loss : 0.3317122987083663 records : 6416\n",
      "train_loss : 0.3321027680190499 records : 8016\n",
      "train_loss : 0.3324630442455089 records : 9616\n",
      "train_loss : 0.33223108881380353 records : 11216\n",
      "train_loss : 0.3323295929012227 records : 12816\n",
      "train_loss : 0.3323840326262632 records : 14416\n",
      "train_loss : 0.3327432540091839 records : 16016\n",
      "train_loss : 0.33290803751871867 records : 17616\n",
      "train_loss : 0.3326759913581099 records : 19216\n",
      "train_loss : 0.33275097956756367 records : 20816\n",
      "train_loss : 0.3326422791452088 records : 22416\n",
      "train_loss : 0.33254480328185015 records : 24016\n",
      "val_loss :  0.3576221466064453 records : 16\n",
      "val_loss :  0.38466731274482047 records : 1616\n",
      "val_loss :  0.3864475838580535 records : 3216\n",
      "val_loss :  0.38654126994237553 records : 4816\n",
      "val_loss :  0.3869059363802769 records : 6416\n",
      "val_loss :  0.3873712603084579 records : 8016\n",
      "val_loss :  0.38753784823536674 records : 9616\n",
      "Epoch: 10 | Time: 5m 43s\n",
      "\tTrain Loss: 0.333 | Train PPL:   1.395 | Train F1: 7.864\n",
      "\t Val. Loss: 0.388 |  Val. PPL:   1.473 | Train F1: 5.525\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_f1 = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss, valid_f1 = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "        \n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Train F1: {train_f1*100:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f} | Val. F1: {valid_f1*100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss :  0.40284645557403564 records : 16\n",
      "val_loss :  0.3827825684358578 records : 1616\n",
      "val_loss :  0.3851188661447212 records : 3216\n",
      "| Test Loss: 0.388 | Test PPL:   1.474 | Test F1: 6.872\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tut1-model.pt'))\n",
    "model.to(device)\n",
    "test_loss, test_f1 = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} | Test F1: {test_f1*100:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk> i , , the the the the ,\n"
     ]
    }
   ],
   "source": [
    "example_sentence = \"i agree ,it 's one of the gems that we get each year .\"\n",
    "doc = spacy_en(example_sentence)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "input_example = SRC.process([tokens])\n",
    "# print(input_example)\n",
    "# print(SRC.vocab.itos[input_example[0][0]])\n",
    "tgt = torch.zeros(50,dtype=torch.long)\n",
    "\n",
    "tgt[0] = TRG.vocab.stoi[TRG.init_token]\n",
    "tgt = tgt.unsqueeze(1)\n",
    "cpu_mod = model.cpu()\n",
    "output = cpu_mod(input_example, tgt, 0.).squeeze()\n",
    "# print(output.shape)\n",
    "\n",
    "def devectorize(vec):\n",
    "  idxs = torch.nonzero(vec)\n",
    "  if len(idxs) == 0:\n",
    "    return 0\n",
    "  else:\n",
    "    sm = torch.softmax(vec, dim=0)\n",
    "    idx = sm.argmax(0)\n",
    "#     print(idx)\n",
    "    return idx\n",
    "\n",
    "outvec = [TRG.vocab.itos[devectorize(token)] for token in output]\n",
    "\n",
    "sentence = []\n",
    "for token in outvec:\n",
    "  if token == TRG.eos_token:\n",
    "    break\n",
    "  sentence.append(token)\n",
    "\n",
    "print(\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
